{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b602337d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from lightning.pytorch import Trainer, seed_everything, LightningDataModule\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ec1af-ca88-4404-8c93-66d846081cfd",
   "metadata": {},
   "source": [
    "Данные предварительно обработаны: ко всем признакам, содержащим значение 0, прибавлена единица, так как 0 используется в качестве паддинга последовательностей.  \n",
    "Обучающая выборка разбита на 10 файлов (id клиентов разбиты на 10 равных частей), из этих файлов удалены значения id, чтобы можно было считать в формате int16 и хранить все данные в оперативной памяти.  \n",
    "Также созданы таблицы с индексами (id клиента - номер строки в файле с данными, которая относится к нему) для извлечения признаков по id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce192695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_emb_size(categories):\n",
    "    size = len(categories) + 1\n",
    "    return (size, int(np.sqrt(size + 1)))\n",
    "\n",
    "\n",
    "class FeaturesMaster(object):\n",
    "    \n",
    "    def __init__(self, base_path, features=None, cat_features=None, num_features=None):\n",
    "\n",
    "        self.base_path = base_path\n",
    "\n",
    "        with open(os.path.join(base_path, 'cat_count.json'), 'r') as f:\n",
    "            self.features_dict = json.load(f)\n",
    "\n",
    "        with open(os.path.join(base_path, 'cat_count_test.json'), 'r') as f:\n",
    "            self.features_dict_test = json.load(f)\n",
    "        \n",
    "        self.features = features if features is not None else [\n",
    "            'rn', 'pre_since_opened', 'pre_since_confirmed', 'pre_pterm',\n",
    "            'pre_fterm', 'pre_till_pclose', 'pre_till_fclose',\n",
    "            'pre_loans_credit_limit', 'pre_loans_next_pay_summ',\n",
    "            'pre_loans_outstanding', 'pre_loans_total_overdue',\n",
    "            'pre_loans_max_overdue_sum', 'pre_loans_credit_cost_rate', 'pre_loans5',\n",
    "            'pre_loans530', 'pre_loans3060', 'pre_loans6090', 'pre_loans90',\n",
    "            'is_zero_loans5', 'is_zero_loans530', 'is_zero_loans3060',\n",
    "            'is_zero_loans6090', 'is_zero_loans90', 'pre_util', 'pre_over2limit',\n",
    "            'pre_maxover2limit', 'is_zero_util', 'is_zero_over2limit',\n",
    "            'is_zero_maxover2limit', 'enc_paym_0', 'enc_paym_1', 'enc_paym_2',\n",
    "            'enc_paym_3', 'enc_paym_4', 'enc_paym_5', 'enc_paym_6', 'enc_paym_7',\n",
    "            'enc_paym_8', 'enc_paym_9', 'enc_paym_10', 'enc_paym_11', 'enc_paym_12',\n",
    "            'enc_paym_13', 'enc_paym_14', 'enc_paym_15', 'enc_paym_16',\n",
    "            'enc_paym_17', 'enc_paym_18', 'enc_paym_19', 'enc_paym_20',\n",
    "            'enc_paym_21', 'enc_paym_22', 'enc_paym_23', 'enc_paym_24',\n",
    "            'enc_loans_account_holder_type', 'enc_loans_credit_status',\n",
    "            'enc_loans_credit_type', 'enc_loans_account_cur', 'pclose_flag',\n",
    "            'fclose_flag'\n",
    "        ]\n",
    "        self.cat_features = cat_features if cat_features is not None else [\n",
    "            'rn', 'pre_since_opened', 'pre_since_confirmed', 'pre_pterm',\n",
    "            'pre_fterm', 'pre_till_pclose', 'pre_till_fclose',\n",
    "            'pre_loans_credit_limit', 'pre_loans_next_pay_summ',\n",
    "            'pre_loans_outstanding', 'pre_loans_total_overdue',\n",
    "            'pre_loans_max_overdue_sum', 'pre_loans_credit_cost_rate',\n",
    "            'is_zero_loans5', 'is_zero_loans530', 'is_zero_loans3060',\n",
    "            'is_zero_loans6090', 'is_zero_loans90', 'pre_util', 'pre_over2limit',\n",
    "            'pre_maxover2limit', 'is_zero_util', 'is_zero_over2limit',\n",
    "            'is_zero_maxover2limit', 'enc_paym_0', 'enc_paym_1', 'enc_paym_2',\n",
    "            'enc_paym_3', 'enc_paym_4', 'enc_paym_5', 'enc_paym_6', 'enc_paym_7',\n",
    "            'enc_paym_8', 'enc_paym_9', 'enc_paym_10', 'enc_paym_11', 'enc_paym_12',\n",
    "            'enc_paym_13', 'enc_paym_14', 'enc_paym_15', 'enc_paym_16',\n",
    "            'enc_paym_17', 'enc_paym_18', 'enc_paym_19', 'enc_paym_20',\n",
    "            'enc_paym_21', 'enc_paym_22', 'enc_paym_23', 'enc_paym_24',\n",
    "            'enc_loans_account_holder_type', 'enc_loans_credit_status',\n",
    "            'enc_loans_credit_type', 'enc_loans_account_cur', 'pclose_flag',\n",
    "            'fclose_flag'\n",
    "        ]\n",
    "\n",
    "        self.num_features = num_features if num_features is not None else [\n",
    "            'pre_loans5', 'pre_loans530', 'pre_loans3060', 'pre_loans6090', 'pre_loans90'\n",
    "        ]\n",
    "        \n",
    "        self.size_features = len(self.features)\n",
    "        self.size_cat_features = len(self.cat_features)\n",
    "        self.size_num_features = len(self.num_features)\n",
    "        \n",
    "        assert set(self.features) == set(self.cat_features) | set(self.num_features)\n",
    "        assert self.size_cat_features + self.size_num_features == self.size_features\n",
    "        assert self.size_cat_features + self.size_num_features == len(set(self.cat_features) | set(self.num_features))\n",
    "\n",
    "    def get_prelayer_params(self):\n",
    "        num = len(self.features)\n",
    "        cat_idx = [i for i in range(num) if self.features[i] in self.cat_features]\n",
    "        num_idx = [i for i in range(num) if self.features[i] in self.num_features]\n",
    "        embeddings_param = [get_emb_size(self.features_dict[self.features[i]]['categories']) for i in cat_idx]\n",
    "        return { 'cat_idx': cat_idx, 'num_idx': num_idx, 'embeddings_param': embeddings_param }\n",
    "\n",
    "    def get_features(self):\n",
    "        return self.features\n",
    "    \n",
    "    def print_size(self):\n",
    "        print(f'features number is {self.size_features}')\n",
    "        print(f'categorical features number is {self.size_cat_features}')\n",
    "        print(f'numerical features number is {self.size_num_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e26bd762",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PklScoringDataset(Dataset):\n",
    "\n",
    "    def __init__(self, files, index, idx_tables, targets, split_file_idx):\n",
    "        self.num_files = len(files)\n",
    "        self.files = files\n",
    "        self.idx_tables = idx_tables\n",
    "        self.index = index\n",
    "        self.targets = targets\n",
    "        size = sum(f.shape[0] for f in files)\n",
    "        part_size = size // self.num_files\n",
    "        self.split_file_idx = split_file_idx\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item_id = self.index[index]\n",
    "        file_num = (self.split_file_idx < item_id).sum()\n",
    "        item_idx = self.idx_tables[file_num].loc[item_id][['idx']].to_numpy().ravel()\n",
    "        item = self.files[file_num][item_idx]\n",
    "        item = torch.LongTensor(item)\n",
    "        return item, torch.LongTensor([self.targets[item_id]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "\n",
    "class PklScoringDatasetPredict(Dataset):\n",
    "\n",
    "    def __init__(self, file, index, idx_table):\n",
    "        self.file = file\n",
    "        self.idx_table = idx_table\n",
    "        self.index = index\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item_id = self.index[index]\n",
    "        item_idx = self.idx_table.loc[item_id][['idx']].to_numpy().ravel()\n",
    "        item = self.file[item_idx]\n",
    "        item = torch.LongTensor(item)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "\n",
    "def collate_function(batch):\n",
    "\n",
    "    items, targets, mask = [], [], []\n",
    "    for item, target in batch:\n",
    "        items.append(item)\n",
    "        targets.append(target)\n",
    "        mask.append(torch.full(size=(len(item),), fill_value=True))\n",
    "\n",
    "    items = torch.nn.utils.rnn.pad_sequence(items, batch_first=True, padding_value=0)\n",
    "    mask = torch.nn.utils.rnn.pad_sequence(mask, batch_first=True, padding_value=False)\n",
    "    targets = torch.Tensor(targets)\n",
    "\n",
    "    return dict(tensor=items, mask=mask, targets=targets)\n",
    "\n",
    "\n",
    "def collate_function_predict(batch):\n",
    "    \n",
    "    mask = list(map(lambda it: torch.full(size=(len(it),), fill_value=True), batch))\n",
    "    items = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    mask = torch.nn.utils.rnn.pad_sequence(mask, batch_first=True, padding_value=False)\n",
    "    return dict(tensor=items, mask=mask, targets=None)\n",
    "\n",
    "\n",
    "class HistoryDataModule(LightningDataModule):\n",
    "    \n",
    "    def __init__(\n",
    "        self, data_dir, features_names, batch_size=1024, num_workers=0,\n",
    "        train_size=None, val_size=None, test_size=None, num_train_files=10,\n",
    "        test_perc=0.15, val_perc=0.015\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.features_names = features_names\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.num_train_files = num_train_files\n",
    "        self.test_perc = test_perc\n",
    "        self.val_perc = test_perc\n",
    "        \n",
    "        self.targets = None\n",
    "        self.train_idx = None\n",
    "        self.valid_idx = None\n",
    "        self.test_idx = None\n",
    "        self.predict_idx = None\n",
    "        \n",
    "        self.train_dataset = None\n",
    "        self.valid_dataset = None\n",
    "        self.test_dataset = None\n",
    "        self.predict_dataset = None\n",
    "        \n",
    "        self.train_file = None\n",
    "        self.test_file = None\n",
    "        \n",
    "        self.set_targets(train_size, val_size, test_size)\n",
    "        train_size = self.targets.size\n",
    "        part_size = train_size // self.num_train_files\n",
    "        self.split_file_idx = np.arange(part_size - 1, train_size - 1, part_size)\n",
    "    \n",
    "    def set_targets(self, train_size, val_size, test_size):\n",
    "        \n",
    "        train_targets = pd.read_csv(os.path.join(self.data_dir, 'train_target.csv'), index_col=0)\n",
    "        test_targets = pd.read_csv(os.path.join(self.data_dir, 'test_target.csv'), index_col=0)\n",
    "        \n",
    "        self.targets = train_targets['flag'].to_numpy()\n",
    "        \n",
    "        self.predict_idx = test_targets.index.to_numpy()\n",
    "        idx = train_targets.index.to_numpy()\n",
    "\n",
    "        test_size_ = int(idx.size * self.test_perc)\n",
    "        val_size_ = int(idx.size * self.val_perc)\n",
    "        train_idx, self.test_idx = train_test_split(idx, test_size=test_size_, shuffle=False)\n",
    "        self.train_idx, self.valid_idx = train_test_split(train_idx, test_size=val_size_, shuffle=True)\n",
    "        \n",
    "        if train_size is not None:\n",
    "            self.train_idx = self.train_idx[:train_size]\n",
    "            \n",
    "        if val_size is not None:\n",
    "            self.valid_idx = self.valid_idx[:val_size]\n",
    "            \n",
    "        if test_size is not None:\n",
    "            self.test_idx = self.test_idx[:test_size]\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == 'fit' or (stage == 'test' and len(self.train_files) == 0):\n",
    "            self.train_files = []\n",
    "            self.train_idx_tables = []\n",
    "            for i in range(self.num_train_files):\n",
    "                file_path = os.path.join(self.data_dir, f'train_part_{i}.csv')\n",
    "                table_path = os.path.join(self.data_dir, f'train_table_part_{i}.csv')\n",
    "                file = pd.read_csv(file_path, dtype=np.int16)[self.features_names].to_numpy()\n",
    "                file_table = pd.read_csv(table_path, index_col=0)\n",
    "                self.train_files.append(file)\n",
    "                self.train_idx_tables.append(file_table)\n",
    "        elif stage == 'predict':\n",
    "            test_path = os.path.join(self.data_dir, 'test_data.csv')\n",
    "            table_path = os.path.join(self.data_dir, 'test_idx_table.csv')\n",
    "            self.test_file = pd.read_csv(test_path, dtype=np.int16)[self.features_names].to_numpy()\n",
    "            self.test_idx_table = pd.read_csv(table_path, index_col=0)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        train_targets = self.targets[self.train_idx]\n",
    "        zeros_p = train_targets.sum() / train_targets.size\n",
    "        weights = np.where(train_targets == 1, 1 - zeros_p, zeros_p)\n",
    "        \n",
    "        self.train_dataset = PklScoringDataset(\n",
    "            files=self.train_files, index=self.train_idx, targets=self.targets,\n",
    "            idx_tables=self.train_idx_tables, split_file_idx=self.split_file_idx\n",
    "        )\n",
    "        sampler = WeightedRandomSampler(weights, 2 * int(train_targets.sum()), replacement=False)\n",
    "        \n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=self.batch_size, sampler=sampler,\n",
    "            num_workers=self.num_workers, collate_fn=collate_function\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        self.valid_dataset = PklScoringDataset(\n",
    "            files=self.train_files, index=self.valid_idx, targets=self.targets,\n",
    "            idx_tables=self.train_idx_tables, split_file_idx=self.split_file_idx\n",
    "        )\n",
    "        return DataLoader(\n",
    "            self.valid_dataset, batch_size=self.batch_size, shuffle=False,\n",
    "            num_workers=self.num_workers, collate_fn=collate_function\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        self.test_dataset = PklScoringDataset(\n",
    "            files=self.train_files, index=self.test_idx, targets=self.targets,\n",
    "            idx_tables=self.train_idx_tables, split_file_idx=self.split_file_idx\n",
    "        )\n",
    "        return DataLoader(\n",
    "            self.test_dataset, batch_size=self.batch_size, shuffle=False,\n",
    "            num_workers=self.num_workers, collate_fn=collate_function\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        self.predict_dataset = PklScoringDatasetPredict(\n",
    "            file=self.test_file, index=self.predict_idx, idx_table=self.test_idx_table\n",
    "        )\n",
    "        return DataLoader(\n",
    "            self.predict_dataset, batch_size=self.batch_size, shuffle=False,\n",
    "            num_workers=self.num_workers, collate_fn=collate_function_predict\n",
    "        )\n",
    "\n",
    "    def teardown(self, stage: str):\n",
    "        if stage == 'fit':\n",
    "            self.train_dataset = None\n",
    "            self.valid_dataset = None\n",
    "        if stage == 'test':\n",
    "            self.train_files = None\n",
    "            self.train_idx_tables = None\n",
    "            self.test_dataset = None\n",
    "        elif stage == 'predict':\n",
    "            self.test_file = None\n",
    "            self.predict_dataset = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "144189fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PreDataBlock(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, cat_idx, num_idx, embeddings_param, output_size):\n",
    "        super().__init__()\n",
    "        assert len(embeddings_param) == len(cat_idx)\n",
    "        self.cat_idx = cat_idx\n",
    "        self.num_idx =  num_idx\n",
    "        self.embeddings_param = embeddings_param\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        layer_output_size = sum([_[1] for _ in embeddings_param]) + len(num_idx)\n",
    "        self.emb_layer = nn.ModuleList(\n",
    "            [nn.Embedding(inp, outp, padding_idx=0) for inp, outp in embeddings_param]\n",
    "        )\n",
    "        self.linear = nn.Linear(layer_output_size, output_size)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        tensor = batch['tensor']\n",
    "        embs_list = [e(tensor[:, :, idx]) for e, idx in zip(self.emb_layer, self.cat_idx)]\n",
    "        embs = torch.cat(embs_list, dim=-1)\n",
    "        all_features = torch.cat([embs, tensor[:, :, self.num_idx]], dim=-1)\n",
    "        next_state = self.linear(all_features)\n",
    "        return dict(tensor=next_state, mask=batch['mask'], targets=batch['targets'])\n",
    "\n",
    "\n",
    "class SelfAttentionLayer(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_size, num_heads=8, dropout=0.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.cls_token = nn.Parameter(torch.rand(input_size))\n",
    "        self.mha = nn.MultiheadAttention(input_size, num_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        batch_size = batch['tensor'].shape[0]\n",
    "        with_token = torch.cat([self.cls_token.repeat((batch_size, 1, 1)), batch['tensor']], dim=1)\n",
    "        new_mask = torch.cat([torch.full((batch_size, 1), fill_value=True), batch['mask']], dim=1)\n",
    "        next_state, _ = self.mha(with_token, with_token, with_token, key_padding_mask=~new_mask)\n",
    "        return dict(tensor=next_state, mask=new_mask, targets=batch['targets'])\n",
    "\n",
    "\n",
    "class LSTM(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=True, dropout=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, batch_first=True,\n",
    "            num_layers=num_layers, bidirectional=bidirectional, dropout=dropout\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        lengths = batch['mask'].sum(-1).detach().cpu()\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            batch['tensor'], lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        output, (h, c) = self.lstm(packed)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        return dict(tensor=output, mask=batch['mask'], targets=batch['targets'])\n",
    "\n",
    "class GRU(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=True, dropout=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size, hidden_size, batch_first=True,\n",
    "            num_layers=num_layers, bidirectional=bidirectional, dropout=dropout\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        lengths = batch['mask'].sum(-1).detach().cpu()\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            batch['tensor'], lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        output, h = self.gru(packed)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        return dict(tensor=output, mask=batch['mask'], targets=batch['targets'])\n",
    "\n",
    "class ResidualLSTM(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=True, dropout=0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.lstm = LSTM(\n",
    "            input_size, hidden_size, num_layers=num_layers,\n",
    "            bidirectional=bidirectional, dropout=dropout\n",
    "        )\n",
    "        D = 2 if bidirectional else 1\n",
    "        self.linear =  nn.Sequential(\n",
    "            nn.Linear(hidden_size * D, input_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        x = batch['tensor']\n",
    "        lstm_out = self.lstm(batch)['tensor']\n",
    "        after_linear = self.linear(lstm_out)\n",
    "        next_state = x + after_linear\n",
    "        return dict(tensor=next_state, mask=batch['mask'], targets=batch['targets'])\n",
    "    \n",
    "    \n",
    "class Squeezer(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, squeezer_type='positional', pos=0, pooling_types=['mean']):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        assert squeezer_type in ['positional', 'pooling']\n",
    "        assert isinstance(pos, int)\n",
    "        assert isinstance(pooling_types, list)\n",
    "\n",
    "        self.squeezer_type = squeezer_type\n",
    "        self.pos = pos\n",
    "        self.pooling_types = pooling_types\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        if self.squeezer_type == 'positional':\n",
    "            next_state = batch['tensor'][:, self.pos, :]\n",
    "        elif self.squeezer_type == 'pooling':\n",
    "            poolings = []\n",
    "            for pooling_type in self.pooling_types:\n",
    "                if pooling_type == 'mean':\n",
    "                    poolings.append(batch['tensor'].mean(1))\n",
    "                elif pooling_type == 'max':\n",
    "                    poolings.append(batch['tensor'].max(1)[0])\n",
    "                elif pooling_type == 'min':\n",
    "                    poolings.append(batch['tensor'].min(1)[0])\n",
    "            next_state = torch.cat(poolings, dim=-1)\n",
    "        return dict(tensor=next_state, targets=batch['targets'])\n",
    "\n",
    "\n",
    "class ClassificationLayer(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_size, num_layers, hidden_size, dropout=0.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_size), nn.ReLU(), nn.Dropout(p=dropout)]\n",
    "        cur_size = hidden_size\n",
    "        for i in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(cur_size, cur_size // 2))\n",
    "            cur_size = cur_size // 2\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "        layers.append(nn.Linear(cur_size, 1))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        next_state = self.layers(batch['tensor']).flatten()\n",
    "        return dict(tensor=next_state, targets=batch['targets'])\n",
    "\n",
    "\n",
    "class Net(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, params):\n",
    "\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.layers = self.configure_net(params)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.layers(batch)\n",
    "    \n",
    "    def configure_net(self, params):\n",
    "        \n",
    "        embed_dim = params['dim']\n",
    "        layers = [PreDataBlock(output_size=embed_dim, **params['prelayer'])]\n",
    "\n",
    "        if params['type'] == 'lstm':\n",
    "            lstm_param = params['lstm']\n",
    "            layers.append(LSTM(input_size=embed_dim, **lstm_param))\n",
    "            embed_dim = lstm_param['hidden_size'] * (2 if lstm_param['bidirectional'] else 1)\n",
    "        elif params['type'] == 'attention':\n",
    "            layers.append(SelfAttentionLayer(input_size=embed_dim, **params['attention']))\n",
    "        elif params['type'] == 'residual_lstm':\n",
    "            layers.append(ResidualLSTM(input_size=embed_dim, **params['lstm']))\n",
    "        elif params['type'] == 'gru':\n",
    "            lstm_param = params['lstm']\n",
    "            layers.append(GRU(input_size=embed_dim, **lstm_param))\n",
    "            embed_dim = lstm_param['hidden_size'] * (2 if lstm_param['bidirectional'] else 1)\n",
    "        \n",
    "        if params['squeezer']['squeezer_type'] == 'positional':\n",
    "            after_squeezer_dim = embed_dim\n",
    "        elif params['squeezer']['squeezer_type'] == 'pooling':\n",
    "            after_squeezer_dim = embed_dim * len(params['squeezer']['pooling_types'])\n",
    "            \n",
    "        layers.extend([\n",
    "            Squeezer(**params['squeezer']),\n",
    "            ClassificationLayer(input_size=after_squeezer_dim, **params['classifier'])\n",
    "        ])\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300d1a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ScoringModule(pl.LightningModule):\n",
    "  \n",
    "    def __init__(self, params):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.net = Net(params['net'])\n",
    "        self.loss_func = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.net(batch)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logits = self(batch)['tensor']\n",
    "        loss = self.loss_func(logits, batch['targets'])\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.training_step_outputs.append(torch.stack([logits, batch['targets']], dim=-1))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self(batch)['tensor']\n",
    "        loss = self.loss_func(logits, batch['targets'])\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.validation_step_outputs.append(torch.stack([logits, batch['targets']], dim=-1))\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        logits = self(batch)['tensor']\n",
    "        loss = self.loss_func(logits, batch['targets'])\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.test_step_outputs.append(torch.stack([logits, batch['targets']], dim=-1))\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        logits = self(batch)['tensor']\n",
    "        return logits\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.net.parameters(), lr=self.params['optimizer']['lr'])\n",
    "        return optimizer\n",
    "        \n",
    "    def on_train_epoch_end(self):\n",
    "        all_preds = torch.cat(self.training_step_outputs, dim=0).detach()\n",
    "        roc_auc = roc_auc_score(all_preds[:, 1].int(), all_preds[:, 0])\n",
    "        self.log('train_roc_auc', roc_auc, prog_bar=True)\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        all_preds = torch.cat(self.validation_step_outputs, dim=0)\n",
    "        roc_auc = roc_auc_score(all_preds[:, 1].int(), all_preds[:, 0])\n",
    "        self.log('val_roc_auc', roc_auc, prog_bar=True)\n",
    "        self.validation_step_outputs.clear()\n",
    "        \n",
    "    def on_test_epoch_end(self):\n",
    "        all_preds = torch.cat(self.test_step_outputs, dim=0)\n",
    "        roc_auc = roc_auc_score(all_preds[:, 1].int(), all_preds[:, 0])\n",
    "        self.log('test_roc_auc', roc_auc, prog_bar=True)\n",
    "        self.test_step_outputs.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b31eb6-9a1a-4170-844a-177ae34b71a1",
   "metadata": {},
   "source": [
    "**Этот ноутбук - пример!**  \n",
    "В реальности использовалась вся обучающая выборка без ограничений train_size=1000, val_size=100, test_size=100, а число воркеров устанавливалось в зависимости от количества доступных cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb2dd174",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "base_path = 'C:/Users/User/Desktop/MIPT_Alpha'\n",
    "data_path = os.path.join(base_path, 'data')\n",
    "fmaster = FeaturesMaster(base_path=data_path)\n",
    "features_names = fmaster.get_features()\n",
    "datamodule = HistoryDataModule(\n",
    "    data_path, batch_size=256, num_workers=0, features_names=features_names, train_size=1000, val_size=100, test_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1836a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'lstm_1'\n",
    "\n",
    "params = {\n",
    "    'optimizer': {'lr' : 0.001},\n",
    "    'net': {\n",
    "        'type': 'lstm',\n",
    "        'dim': 64,\n",
    "        'prelayer': fmaster.get_prelayer_params(),\n",
    "        'lstm': {\n",
    "            'hidden_size': 64,\n",
    "            'num_layers': 1,\n",
    "            'bidirectional': True,\n",
    "            'dropout': 0.0\n",
    "        },\n",
    "        'squeezer': { 'squeezer_type': 'pooling', 'pooling_types': ['mean', 'max'] },\n",
    "        'classifier': { 'num_layers': 2, 'hidden_size': 16, 'dropout': 0.05 }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53675ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = ScoringModule(params)\n",
    "checkpoint_roc_auc_callback = ModelCheckpoint(\n",
    "    monitor='val_roc_auc', mode='max', filename='roc_auc-{epoch}-{step}-{val_roc_auc:.4f}', save_top_k=-1,\n",
    "    dirpath=os.path.join(base_path, 'logs', experiment_name, params['net']['type'])\n",
    ")\n",
    "checkpoint_loss_callback = ModelCheckpoint(\n",
    "    monitor='val_loss', mode='min', filename='loss-{epoch}-{step}-{val_loss:.4f}', save_top_k=-1,\n",
    "    dirpath=os.path.join(base_path, 'logs', experiment_name, params['net']['type'])\n",
    ")\n",
    "early_stopping_callback = EarlyStopping(monitor='val_roc_auc', mode='max', min_delta=0.0, patience=3, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e928e934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    min_epochs=2,\n",
    "    max_epochs=30,\n",
    "    num_sanity_val_steps=0,\n",
    "    check_val_every_n_epoch=1,\n",
    "    \n",
    "    accelerator='cpu',\n",
    "    deterministic=True,\n",
    "    callbacks=[\n",
    "        checkpoint_roc_auc_callback,\n",
    "        checkpoint_loss_callback,\n",
    "        early_stopping_callback\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae40a37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:612: UserWarning: Checkpoint directory C:\\Users\\User\\Desktop\\MIPT_Alpha\\logs\\lstm_1\\lstm exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | net       | Net               | 81.8 K\n",
      "1 | loss_func | BCEWithLogitsLoss | 0     \n",
      "------------------------------------------------\n",
      "81.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "81.8 K    Total params\n",
      "0.327     Total estimated model params size (MB)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e49d8ae207464787d57a17c94ce0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model=module, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f76e45a6-b20e-41be-9575-a2fcf4e2b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_dict = dict()\n",
    "for path in glob.glob(os.path.join(base_path, 'logs', experiment_name, params['net']['type'] + '/*')):\n",
    "    if 'roc_auc' in path:\n",
    "        name = re.search(r'roc_auc-epoch=.+', path)[0]\n",
    "        score = re.search(r'0\\.\\d{4}', path)[0]\n",
    "        roc_auc_dict[name] = float(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68cbe95a-df5a-48e3-a898-c463aea8a1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc-epoch=15-step=16-val_roc_auc=0.7388.ckpt 0.7388\n"
     ]
    }
   ],
   "source": [
    "best_name, best_score = max(roc_auc_dict.items(), key=lambda x: x[1])\n",
    "print(best_name, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5fc28a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "Restoring states from the checkpoint path at C:/Users/User/Desktop/MIPT_Alpha\\logs\\lstm_1\\lstm\\roc_auc-epoch=15-step=16-val_roc_auc=0.7388.ckpt\n",
      "Loaded model weights from the checkpoint at C:/Users/User/Desktop/MIPT_Alpha\\logs\\lstm_1\\lstm\\roc_auc-epoch=15-step=16-val_roc_auc=0.7388.ckpt\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8ba27e6b97497da493c1945f204b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datamodule.setup('test')\n",
    "test_preds = trainer.predict(\n",
    "    module,\n",
    "    dataloaders=datamodule.test_dataloader(), return_predictions=True,\n",
    "    ckpt_path=os.path.join(base_path, 'logs', experiment_name, params['net']['type'], best_name)\n",
    ")\n",
    "datamodule.teardown('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20744ab3-2562-4187-b9fb-c76fcf97a83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36979166666666663"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = torch.cat(test_preds).flatten()\n",
    "roc_auc_score(datamodule.targets[datamodule.test_idx], test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f4f0075-5cda-40ed-b668-be9156a2bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(base_path, 'results/test_1.pkl'), 'wb') as f:\n",
    "    pickle.dump(test_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "805cb9d9-b27f-4909-9797-bf4ca686bd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "Restoring states from the checkpoint path at C:/Users/User/Desktop/MIPT_Alpha\\logs\\lstm_1\\lstm\\roc_auc-epoch=15-step=16-val_roc_auc=0.7388.ckpt\n",
      "Loaded model weights from the checkpoint at C:/Users/User/Desktop/MIPT_Alpha\\logs\\lstm_1\\lstm\\roc_auc-epoch=15-step=16-val_roc_auc=0.7388.ckpt\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b72f86477e4118bab603c0965231ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(\n",
    "    module,\n",
    "    datamodule=datamodule, return_predictions=True,\n",
    "    ckpt_path=os.path.join(base_path, 'logs', experiment_name, params['net']['type'], best_name)\n",
    ")\n",
    "\n",
    "predictions = torch.cat(predictions).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10e0cc76-34af-4c01-8a56-185fcf537d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(base_path, 'results/predictions_1.pkl'), 'wb') as f:\n",
    "    pickle.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa078b-e643-4266-a93d-3b588fb88d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
